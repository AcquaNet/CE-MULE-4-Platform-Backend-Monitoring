# Docker Compose Configuration for ELK Stack + APISIX Gateway
#
# This is the base configuration running with HTTP (no SSL/TLS).
# For production deployments with SSL/TLS, use the SSL override:
#
#   docker-compose -f docker-compose.yml -f docker-compose.ssl.yml up -d
#
# Prerequisites for SSL:
#   1. Generate certificates: ./scripts/generate-certs.sh
#   2. OR setup Let's Encrypt: ./scripts/setup-letsencrypt.sh --domain yourdomain.com
#   3. Enable SSL in .env: SSL_ENABLED=true
#   4. See docs/SSL_TLS_SETUP.md for complete guide
#

version: '3.8'

services:
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:${ELASTIC_VERSION:-8.11.3}
    container_name: elasticsearch
    environment:
      - node.name=${NODE_NAME:-elasticsearch}
      - cluster.name=${ELASTIC_CLUSTER_NAME:-elk-cluster}
      - discovery.type=${DISCOVERY_TYPE:-single-node}
      - bootstrap.memory_lock=true
      - ES_JAVA_OPTS=${ES_JAVA_OPTS:--Xms2g -Xmx2g}
      - xpack.security.enabled=${XPACK_SECURITY_ENABLED:-true}
      - xpack.security.enrollment.enabled=true
      - xpack.security.http.ssl.enabled=${XPACK_SECURITY_HTTP_SSL_ENABLED:-false}
      - xpack.security.transport.ssl.enabled=${XPACK_SECURITY_TRANSPORT_SSL_ENABLED:-false}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
      - elasticsearch-backups:${SNAPSHOT_REPOSITORY_PATH:-/mnt/elasticsearch-backups}
    # Ports removed - access via APISIX gateway at /elasticsearch
    # Internal access still available via container name
    expose:
      - "9200"
      - "9300"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.10
      ce-base-network:
    healthcheck:
      test: ["CMD-SHELL", "curl -f -u elastic:${ELASTIC_PASSWORD} http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  kibana:
    image: docker.elastic.co/kibana/kibana:${ELASTIC_VERSION:-8.11.3}
    container_name: kibana
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_URL=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=kibana_system
      - ELASTICSEARCH_PASSWORD=${KIBANA_PASSWORD}
      - xpack.security.enabled=${XPACK_SECURITY_ENABLED:-true}
      - xpack.encryptedSavedObjects.encryptionKey=${KIBANA_ENCRYPTION_KEY}
      - xpack.reporting.encryptionKey=${KIBANA_REPORTING_ENCRYPTION_KEY}
      - xpack.fleet.enabled=true
      - SERVER_BASEPATH=/kibana
      - SERVER_REWRITEBASEPATH=false
    # Port removed - access via APISIX gateway at /kibana
    expose:
      - "5601"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.12
      ce-base-network:
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  logstash:
    image: docker.elastic.co/logstash/logstash:${ELASTIC_VERSION:-8.11.3}
    container_name: logstash
    environment:
      - LS_JAVA_OPTS=${LS_JAVA_OPTS:--Xms1g -Xmx1g}
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=${ELASTIC_PASSWORD}
      - ELASTIC_PASSWORD=${ELASTIC_PASSWORD}
      - LOGSTASH_AUTH_TOKEN=${LOGSTASH_AUTH_TOKEN}
    volumes:
      - ./config/logstash/config/logstash.yml:/usr/share/logstash/config/logstash.yml:ro
      - ./config/logstash/pipeline:/usr/share/logstash/pipeline:ro
    # All Logstash access is now routed through APISIX gateway for load balancing support
    # External access via APISIX:
    #   - TCP log ingestion: localhost:9100 (APISIX) -> logstash:5000
    #   - Beats input: localhost:9144 (APISIX) -> logstash:5044
    #   - Monitoring API: http://localhost:9080/logstash (APISIX) -> logstash:9600
    #
    # For direct access (bypassing APISIX), uncomment these ports:
    # ports:
    #   - "5044:5044"      # Beats input (Filebeat, etc.)
    #   - "5000:5000/tcp"  # TCP input for JSON logs
    #   - "5000:5000/udp"  # UDP input for JSON logs (not routed via APISIX)
    expose:
      - "5000"           # TCP/UDP input (internal, via APISIX stream proxy)
      - "5044"           # Beats input (internal, via APISIX stream proxy)
      - "9600"           # Monitoring API (internal, via APISIX HTTP route)
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.11
      ce-base-network:
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9600 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  kibana-setup:
    image: curlimages/curl:latest
    container_name: kibana-setup
    volumes:
      - ./scripts/setup-kibana.sh:/setup-kibana.sh:ro
    networks:
      - ce-base-micronet
      - ce-base-network
    depends_on:
      kibana:
        condition: service_healthy
    command: sh /setup-kibana.sh
    restart: on-failure

  #
  # Elastic APM Server - Application Performance Monitoring
  #
  apm-server:
    image: docker.elastic.co/apm/apm-server:8.10.4
    container_name: apm-server
    command: >
      apm-server -e
        -E apm-server.host=0.0.0.0:8200
        -E output.elasticsearch.hosts=['http://elasticsearch:9200']
        -E output.elasticsearch.username=elastic
        -E output.elasticsearch.password=${ELASTIC_PASSWORD}
        -E output.elasticsearch.protocol=http
        -E apm-server.kibana.enabled=true
        -E apm-server.kibana.host=http://kibana:5601
        -E apm-server.kibana.username=elastic
        -E apm-server.kibana.password=${ELASTIC_PASSWORD}
        -E apm-server.auth.anonymous.enabled=false
        -E apm-server.auth.secret_token=${APM_SECRET_TOKEN}
        -E apm-server.rum.enabled=false
    ports:
      - "8200:8200"      # APM Server HTTP endpoint
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.13
      ce-base-network:
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "pgrep -f apm-server || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always

  apm-setup:
    image: curlimages/curl:latest
    container_name: apm-setup
    volumes:
      - ./scripts/setup-apm.sh:/setup-apm.sh:ro
    networks:
      - ce-base-micronet
      - ce-base-network
    depends_on:
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
      apm-server:
        condition: service_started
    command: sh /setup-apm.sh
    restart: on-failure

  #
  # Apache APISIX - API Gateway and Load Balancer
  #
  etcd:
    image: quay.io/coreos/etcd:v3.5.9
    container_name: etcd
    environment:
      - ETCD_NAME=etcd
      - ETCD_ADVERTISE_CLIENT_URLS=http://etcd:2379
      - ETCD_LISTEN_CLIENT_URLS=http://0.0.0.0:2379
      - ETCD_INITIAL_ADVERTISE_PEER_URLS=http://etcd:2380
      - ETCD_LISTEN_PEER_URLS=http://0.0.0.0:2380
      - ETCD_INITIAL_CLUSTER=etcd=http://etcd:2380
    ports:
      - "2379:2379"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.21
      ce-base-network:
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always

  apisix:
    image: apache/apisix:3.7.0-debian
    container_name: apisix
    environment:
      - APISIX_ADMIN_KEY=${APISIX_ADMIN_KEY:-edd1c9f034335f136f87ad84b625c8f1}
    volumes:
      - ./config/apisix/config/config.yaml:/usr/local/apisix/conf/config.yaml:ro
      - ./config/apisix/apisix.yaml:/usr/local/apisix/conf/apisix.yaml:ro
      - apisix-logs:/usr/local/apisix/logs
    ports:
      - "9080:9080"    # HTTP Gateway
      - "9443:9443"    # HTTPS Gateway
      - "9180:9180"    # Admin API
      - "9091:9091"    # Prometheus Metrics
      - "9092:9092"    # Control API
      - "9100:9100/tcp"  # Stream proxy: Logstash TCP input (routes to logstash:5000)
      - "9144:9144/tcp"  # Stream proxy: Logstash Beats input (routes to logstash:5044)
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.20
      ce-base-network:
    depends_on:
      etcd:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      kibana:
        condition: service_healthy
      logstash:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "test -d /usr/local/apisix || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    restart: always

  apisix-dashboard:
    image: apache/apisix-dashboard:3.0.1-alpine
    container_name: apisix-dashboard
    volumes:
      - ./config/apisix-dashboard/conf.yaml:/usr/local/apisix-dashboard/conf/conf.yaml:ro
    # Dashboard accessible via APISIX at http://localhost:9080/ (root path)
    # Port exposure not needed - all access through APISIX gateway
    expose:
      - "9000"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.22
      ce-base-network:
    depends_on:
      etcd:
        condition: service_healthy
      apisix:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "wget -q --spider http://localhost:9000/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: always

  apisix-setup:
    image: curlimages/curl:latest
    container_name: apisix-setup
    volumes:
      - ./scripts/setup-apisix.sh:/setup-apisix.sh:ro
    networks:
      - ce-base-micronet
      - ce-base-network
    depends_on:
      apisix:
        condition: service_healthy
    command: sh /setup-apisix.sh
    restart: on-failure

  #
  # Prometheus - Metrics Collection and Storage
  #
  prometheus:
    image: prom/prometheus:v2.48.0
    container_name: prometheus
    volumes:
      - ./config/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - ./config/prometheus/rules:/etc/prometheus/rules:ro
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--storage.tsdb.retention.time=${PROMETHEUS_RETENTION:-30d}'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    # Prometheus accessible via APISIX at /prometheus
    expose:
      - "9090"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.23
      ce-base-network:
    depends_on:
      apisix:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always

  #
  # Grafana - Metrics Visualization and Dashboards
  #
  grafana:
    image: grafana/grafana:10.2.2
    container_name: grafana
    environment:
      - GF_SERVER_ROOT_URL=http://localhost:9080/grafana
      - GF_SERVER_SERVE_FROM_SUB_PATH=true
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD}
      - GF_SECURITY_SECRET_KEY=${GRAFANA_SECRET_KEY}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=
      - GF_SECURITY_ALLOW_EMBEDDING=true
      - GF_SECURITY_COOKIE_SAMESITE=none
      - GF_SECURITY_COOKIE_SECURE=false
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
    volumes:
      - ./config/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana-data:/var/lib/grafana
    # Grafana accessible via APISIX at /grafana
    expose:
      - "3000"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.24
      ce-base-network:
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always

  #
  # Alertmanager - Alert Notification Manager
  # Enabled when ALERTING_ENABLED=true in .env
  #
  alertmanager:
    image: prom/alertmanager:v0.26.0
    container_name: alertmanager
    profiles:
      - alerting
    volumes:
      - ./config/alertmanager/alertmanager.yml:/etc/alertmanager/alertmanager.yml:ro
      - alertmanager-data:/alertmanager
    command:
      - '--config.file=/etc/alertmanager/alertmanager.yml'
      - '--storage.path=/alertmanager'
      - '--data.retention=${ALERTMANAGER_RETENTION:-120h}'
      - '--web.external-url=${ALERTMANAGER_EXTERNAL_URL:-http://localhost:9080/alertmanager}'
    # Alertmanager accessible via APISIX at /alertmanager
    expose:
      - "9093"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.25
      ce-base-network:
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9093/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always

  #
  # ElasticSearch Exporter - Exports ElasticSearch metrics to Prometheus
  # Enabled when ELASTICSEARCH_EXPORTER_ENABLED=true in .env
  #
  elasticsearch-exporter:
    image: quay.io/prometheuscommunity/elasticsearch-exporter:v1.6.0
    container_name: elasticsearch-exporter
    profiles:
      - elasticsearch-monitoring
    command:
      - '--es.uri=http://elasticsearch:9200'
      - '--es.all'
      - '--es.indices'
      - '--es.indices_settings'
      - '--es.shards'
      - '--es.snapshots'
      - '--es.cluster_settings'
      - '--es.timeout=30s'
    environment:
      - ES_USERNAME=elastic
      - ES_PASSWORD=${ELASTIC_PASSWORD}
    expose:
      - "9114"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.26
      ce-base-network:
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:9114/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    restart: always

  #
  # Keycloak - Identity and Access Management
  #
  keycloak:
    image: quay.io/keycloak/keycloak:23.0
    container_name: keycloak
    environment:
      - KEYCLOAK_ADMIN=${KEYCLOAK_ADMIN_USER:-admin}
      - KEYCLOAK_ADMIN_PASSWORD=${KEYCLOAK_ADMIN_PASSWORD:-admin}
      - KC_DB=postgres
      - KC_DB_URL=jdbc:postgresql://keycloak-db:5432/keycloak
      - KC_DB_USERNAME=keycloak
      - KC_DB_PASSWORD=${KEYCLOAK_DB_PASSWORD:-keycloak}
      - KC_HOSTNAME=localhost
      - KC_HOSTNAME_PORT=9443
      - KC_HOSTNAME_STRICT=false
      - KC_HOSTNAME_STRICT_HTTPS=false
      - KC_HOSTNAME_ADMIN_URL=https://localhost:9443/auth
      - KC_HTTP_ENABLED=true
      - KC_PROXY_ADDRESS_FORWARDING=true
      - KC_PROXY=edge
      - KC_HEALTH_ENABLED=true
      - KC_HTTP_RELATIVE_PATH=/auth
    command:
      - start-dev
      - --import-realm
    volumes:
      - ./config/keycloak/realms:/opt/keycloak/data/import:ro
      - keycloak-data:/opt/keycloak/data
    # Keycloak accessible via APISIX at /auth
    expose:
      - "8080"
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.27
      ce-base-network:
    depends_on:
      keycloak-db:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "exec 3<>/dev/tcp/127.0.0.1/8080;echo -e 'GET /health/ready HTTP/1.1\r\nhost: http://localhost\r\nConnection: close\r\n\r\n' >&3;if [ $? -eq 0 ]; then echo 'Healthcheck Successful';exit 0;else echo 'Healthcheck Failed';exit 1;fi;"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s
    restart: always

  #
  # Keycloak Database - PostgreSQL
  #
  keycloak-db:
    image: postgres:15-alpine
    container_name: keycloak-db
    environment:
      - POSTGRES_DB=keycloak
      - POSTGRES_USER=keycloak
      - POSTGRES_PASSWORD=${KEYCLOAK_DB_PASSWORD:-keycloak}
    volumes:
      - keycloak-db-data:/var/lib/postgresql/data
    networks:
      ce-base-micronet:
        ipv4_address: 172.42.0.28
      ce-base-network:
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U keycloak"]
      interval: 10s
      timeout: 5s
      retries: 5
    restart: always

volumes:
  elasticsearch-data:
    driver: local
  elasticsearch-backups:
    driver: local
  apisix-logs:
    driver: local
  prometheus-data:
    driver: local
  grafana-data:
    driver: local
  alertmanager-data:
    driver: local
  keycloak-data:
    driver: local
  keycloak-db-data:
    driver: local

networks:
  ce-base-micronet:
    external: true
  ce-base-network:
    external: true
